{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d71e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70a5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  \n",
    "sample_rng = torch.Generator()\n",
    "sample_rng.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21832af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 1024\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "head_dim = hidden_dim // num_heads\n",
    "\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae298657",
   "metadata": {},
   "source": [
    "## CPU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa679df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU from scratch implementation: 752.25ms\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(batch_size, sequence_length, hidden_dim, generator=sample_rng)\n",
    "\n",
    "dt = 0\n",
    "for i in range(n_iter):\n",
    "    t0 = time.time()\n",
    "    wq = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "    wk = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "    wv = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "    wo = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        q = wq(x).view(batch_size, sequence_length, num_heads, head_dim).transpose(1,2)\n",
    "        k = wk(x).view(batch_size, sequence_length, num_heads, head_dim).transpose(1,2)\n",
    "        v = wv(x).view(batch_size, sequence_length, num_heads, head_dim).transpose(1,2)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(batch_size, sequence_length, hidden_dim)\n",
    "        manual_output = wo(y)\n",
    "    t1 = time.time()\n",
    "    dt += (t1 - t0)*1000\n",
    "print(f\"CPU from scratch implementation: {dt/n_iter:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d35ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU pytorch implementation: 701.31ms\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(batch_size, sequence_length, hidden_dim, generator=sample_rng)\n",
    "\n",
    "#warmup\n",
    "multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=0.0)\n",
    "attn_output, attn_output_weights = multihead_attn(x, x, x)\n",
    "\n",
    "dt = 0\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=0.0)\n",
    "    attn_output, attn_output_weights = multihead_attn(x, x, x)\n",
    "    t1 = time.time()\n",
    "    dt += (t1 - t0)*1000   \n",
    "print(f\"CPU pytorch implementation: {dt/n_iter:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78feaa4",
   "metadata": {},
   "source": [
    "## GPU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38677239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU from scratch implementation: 60.79ms\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "x = torch.randn(batch_size, sequence_length, hidden_dim, device=device)\n",
    "\n",
    "dt = 0\n",
    "for i in range(n_iter):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    wq = torch.nn.Linear(hidden_dim, hidden_dim, device=device)\n",
    "    wk = torch.nn.Linear(hidden_dim, hidden_dim, device=device)\n",
    "    wv = torch.nn.Linear(hidden_dim, hidden_dim, device=device)\n",
    "    wo = torch.nn.Linear(hidden_dim, hidden_dim, device=device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        q = wq(x).view(batch_size, sequence_length, num_heads, head_dim).transpose(1,2)\n",
    "        k = wk(x).view(batch_size, sequence_length, num_heads, head_dim).transpose(1,2)\n",
    "        v = wv(x).view(batch_size, sequence_length, num_heads, head_dim).transpose(1,2)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(batch_size, sequence_length, hidden_dim)\n",
    "        manual_output = wo(y)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt += (t1 - t0)*1000\n",
    "print(f\"GPU from scratch implementation: {dt/n_iter:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f81f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU pytorch implementation: 15.98ms\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "x = torch.randn(batch_size, sequence_length, hidden_dim, device=device)\n",
    "\n",
    "#warmup\n",
    "multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=0.0, device=device)\n",
    "attn_output, attn_output_weights = multihead_attn(x, x, x)\n",
    "\n",
    "dt = 0\n",
    "for i in range(n_iter):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=0.0, device=device)\n",
    "    attn_output, attn_output_weights = multihead_attn(x, x, x)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt += (t1 - t0)*1000   \n",
    "print(f\"GPU pytorch implementation: {dt/n_iter:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba3569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
